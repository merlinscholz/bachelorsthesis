\chapter{Grundlagen}
\label{chap:grundlagen}

%\section{Analyse der Marsoberfläche}
%\label{sec:analysedermarsoberflache}

\section{Aufnahmen der Marsoberfläche}
\label{sec:mars_images}

Von der Marsoberfläche existieren verschiedenste Arten von Aufnahmen für unterschiedliche Analysen. Dabei kommt es auf gewisse Parameter, wie \zB die aufgenommene Wellenlänge, den Winkel zur Oberfläche, die Auflösung, Brennweite, Ort der Aufnahme und viele weitere an, ob ein Bild zur Analyse einer gewissen Eigenschaft geeignet ist.

Für die hier genutzten Anwendungsfälle eignen sich Instrumente am besten, die sichtbares Licht aufnehmen. Diese sollten in Graustufen, in diesem Einsatzbereich also panchromatisch, aufgenommen sein, da die Farbe der Oberfläche deren Analyse hier wenig beeinflusst. Der Vorteil von Graustufenaufnahmen besteht darin, dass nur ein drittel des Speichers bei den Berechnungen benötigt wird.

Des weiteren sollten die Bilder eine vergleichsweise hohe Auflösung haben, damit auch kleinere Merkmale gut erkannt werden können, es sollten aber auch weite Aufnahmen sein, damit mit einem Datensatz ein Großteil der Oberfläche analysiert werden kann.

Außerdem eignen sich Fotos gut, die senkrecht zur Oberfläche entstanden sind, so dass diese nicht verzerrt erscheint.

Unter Berücksichtigung dieser Faktoren ergeben sich \ua zwei geeignete Instrumente:
Die \textit{Context Camera (CTX)} des \textit{Mars Reconnaissance Orbiters} der NASA \cite{malin_07} und die \textit{High Resolution Stereo Camera (HRSC)} des \textit{Mars Express Orbiters} der ESA \cite{hrsc}. Für die eigentlichen Analysen werden Aufnahmen der CTX genutzt, da sie einen Großteil der Oberfläche abdecken, während die panchromatischen Bilder der HRSC zur Evaluierung genutzt wurden, da mit ihnen schon mehrere andere Algorithmen getestet wurden.


\iffalse
\section{Clustering}
\label{sec:clustering}


\section{Bildsegmentierung}
\label{sec:segmentierung}

Um in einem Eingabebild, wie \zB einer Aufnahme der Marsoberfläche, verschiedene Oberflächenstrukturen erkennen zu können, bedarf es eines Algorithmus zur Bildsegmentierung. Wie in \cite{bildsegmentierung_14} aufgeführt, beschreibt der Überbegriff der Bildsegmentierung oftmals den Ablauf des Pre-Processing eines Eingabebildes, gefolgt von der eigentlichen \textquote[\cite{bildsegmentierung_14}]{Segmentierung der Bilder in Regionen, Objekte mit geschlossenen Konturen oder Liniensegmente} und, je nach Anwendungsfall, der anschließenden Einordnung der einzelnen Segmente.

\section{Künstliche Intelligenz, maschinelles Lernen und Deep Learning}

\begin{wrapfigure}{r}{0.33\textwidth}
\begin{tikzpicture}
\draw (0,0) ellipse (3 and 2);
\draw (0,-1.5) node[align=center]{\footnotesize Künstliche Intelligenz};
\draw (0.25,0.4) ellipse (2.3 and 1.5);
\draw (0.3,-0.5) node[align=center]{\footnotesize Maschinelles Lernen};
\draw (0.5,0.8) ellipse (1 and 1);
\draw (0.5,0.8) node[align=center]{\footnotesize Deep\\\footnotesize Learning};
\end{tikzpicture}
\captionsetup{format=plain}
\caption{Relation zwischen künstlicher Intelligenz, maschinellem Lernen und Deep Learning, nach \cite[Kap.~1.1]{deeplearning_18}}
\label{fig:ai_ml_dl}
\end{wrapfigure}

Für den Begriff der künstlichen Intelligenz existieren viele Definitionen. Meistens wird mit ihm der Versuch bezeichnet, Maschinen, wie \zB Computern, eine Intelligenz beizubringen, die der des Menschen möglichst nahe sein sollte. Die Definition der Intelligenz ist selbst allerdings auch sehr unterschiedlich, laut einer Definition beschreibt Intelligenz \textquote{\textelp{} das Merkmal, das einem Wesen ermöglicht, angemessen und vorausschauend in seiner Umgebung zu funktionieren.}\footnote{\textquote[\cite{thequest_09}]{\textelp{} and intelligence is that quality that enables an entity to function appropriately and with foresight in its environment.}}

Während man in der klassischen Programmierung zur Lösung eines Problems sowohl Eingabedaten als auch Bearbeitungsregeln benötigt, so ändert sich dies beim maschinellen Lernen, einem Unterbegriff der künstlichen Intelligenz. Hier werden die Bearbeitungsregeln durch schon bekannte Ergebnisse -- passend zu den Eingabedaten -- ersetzt. Durch diese Kombination von Ein- und Ausgabewerten leitet die Maschine in der ersten Phase, auch Training genannt, so selbst die Bearbeitungsregeln ab, und kann sie in der zweiten Phase auf neue, unbekannte Eingabewerte anwenden. Diese Bearbeitungsregeln bestehen aus generierten Formeln und beruhen auf rein statistischen Annahmen darüber, mit welcher Wahrscheinlichkeit eine gewisse Eingabe zu einer anderen gewissen Ausgabe führt. \cite{deeplearning_18} 



Obwohl einfache Versionen neuronaler Netze schon seit vielen Jahrzehnten in der Informatik genutzt werden, wurden mit ihnen in den letzten Jahren neue Durchbrüche in verschiedensten Bereichen erzielt. Dieser Umschwung ist darauf zurückzuführen, dass mit dem Fortschritt der Technik nun auch große Netzwerke effizient ausgeführt werden können.

\section{Convolutional Neural Networks}
\label{sec:cnn}



Ein Convolutional Neural Network beschreibt ein Neuronales Netzwerk, welches eine Convolutional Layer enthält. Eine Convolutional Layer enthält ihre Eingaben von mehreren Datenpunkten der vorherigen Schicht: Das (meist rechteckige) Fenster der Eingabedaten dieser Schicht gleitet somit schrittweise über die Ausgabe der vorherigen Schicht. \cite{schmidhuber_15} Dies hat den Vorteil, dass die Größe der darauf folgenden Schicht verringert wird (was zu einer besseren Performance führt). Ein weiterer Pluspunkt besteht darin, dass insbesondere bei der Bildanalyse gleiche Objekte an verschiedenen Positionen und in verschiedenen Größen besser erkannt werden können. \cite{szegedy_15}

Aus diesen Gründen erfreuen sich Convolutional Neural Networks insbesondere in der Bildanalyse (wie \zB bei der Objekterkennung) großer Beliebtheit.

\fi

\section{Convolutional Neural Networks}
\label{sec:cnn}

Neuronale Netze -- meist synonym mit Deep Learning genutzt -- werden oftmals als eine Weiterentwicklung oder Optimierung des maschinellen Lernens betrachtet: Statt einer Menge von Formeln, die die Approximation einer Funtion erlernen sollen, setzt man hier auf mehrere Schichten, auch Layers genannt, von vergleichsweise einfachen linearen Funktionen, um zur gewünschten Approximation zu kommen. \cite{hardesty_17}

In diesem Kontext wird ein Knoten, auch als Neuron bezeichnet. Ein typisches Neuron verarbeitet einen Eingabevektor indem es das Kreuzprodukt mit eben diesem Vektor und den verschiedenen Gewichtungen berechnet und anschließend einen Bias aufaddiert. Während das Netzwerk trainiert wird, passen die einzelnen Knoten diese zwei Vektoren, also die Gewichtungen und die Biasse an. Diese Gewichtungen sind anfangs zufällig bestimmt. \cite{hardesty_17, cs231n}

Die Aneinanderkettung dieser verschiedenen Schichten führt dazu, dass zwischen ihnen eine immer abstrahiertere Form der Eingabedaten entsteht, damit sind sie in ihrer Funktionsweise an die des menschlichen Gehirns angelehnt, welches sich auch aus einer Großzahl von hintereinandergeschalteten Neuronen zusammensetzt. % TODO Cite

Convolutional Neural Networks beschreiben eine Teilmenge der neuronalen Netze, in der die jeweilige Netzwerkarchitektur mindestens eine Convolutional Layer (auch Faltungsschicht genannt, \vgl Unterabschnitt~\ref{ssec:conv}) enthält. Die Nutzung dieser Convolutional Layer zieht fast immer die Nutzung einer Pooling Layer (Unterabschnitt~\ref{ssec:pooling}) mit sich. \cite{deeplearning_16}

Obwohl Convolutional Neural Networks theoretisch dazu geeignet sind die meisten Daten mit einer gitterähnlichen Struktur zu verarbeiten, erfreuen sie sich im Bereich der Bilddatenanalyse der größten Beliebtheit. \cite[Kap.~9]{deeplearning_16} Eine wichtige Ursache für diese Beliebtheit liegt \bspw in dem Erfolg bei dem Bild-Klassifizierungs-Wettbewerb \textit{ImageNet}. Während dort im Jahr 2011 die Gewinnergruppe mit klassischen Klassifizierungsalgorithmen einen Top-5-Score von $74,3\%$ erzielt hat, wurden im Jahr 2012 mit einem CNN erstmals ein Wert von $83,6\%$ erreicht. Von diesem Punkt an wurde die Bestenliste der darauffolgenden Jahre durch Convolutional Neural Networks dominiert. \cite[Kap.~1]{deeplearning_18}

% TODO Add Automonous Driving for segmentation problems

% TODO Why are they useful? Smaller sized networks

Da CNNs primär auf Bilddateien angewandt werden, wird von diesem Punkt an von einer Bilddatei als Eingabe ausgegangen. Hier beschreiben die ersten beiden Dimensionen die Breite und Höhe, während die dritte Dimension die Farbwerte für den jeweiligen Pixel angibt.

\subsection{Konvolution}
\label{ssec:conv}

Die Konvolution ist eine mathematische Operation auf zwei Funktionen $f(x)$ und $g(y)$, die eine dritte Funktion, die Konvolution $s = f*g$ ergibt, welche beschreibt, wie sich die Verläufe der beiden Funktionen beeinflussen:

\begin{equation}
s(x) = (f*g)(x) = \int_{-\infty}^{\infty} f(y)g(x-y)dy
\end{equation}

In einem Großteil der Anwendungsfälle der Konvolution bestehen die Eingabefunktionen aus einer Eingabefunktion $x$ und einem Kernel $w$, die Ausgabe ist die Merkmalsdimension. In einer praktischen Anwendung wären dies \bspw Messwerte $x(t)$ in Abhängig von der Zeit und einer Gewichtungsfunktion $w(a)$ in Abhängigkeit des Alters der Messung. Für ungültige Zeiten (\bspw Zukunft) gilt $w=0$ \cite[Kap.~9]{deeplearning_16}:

\begin{equation}
s(t) = (x*w)(t) = \int_{-\infty}^{\infty} x(a)w(t-a)da
\end{equation}

Unter der Annahme, dass die Eingabewerte nicht stetig sondern diskret sind (\bspw zeitliche Messwerte in regelmäßigen Abständen) ergibt sich vereinfacht \cite[Kap.~9]{deeplearning_16}:

\begin{equation}
s(t) = (x*w)(t) = \sum_{a=-\infty}^{\infty}x(a)w(t-a)
\end{equation}

Diese Funktion lässt sich für zweidimensionale Eingabedaten, wie \zB eine Bilddatei $I$ und einen zweidimensionalen Kernel $K$ erweitern \cite[Kap.~9]{deeplearning_16}:

\begin{equation}
S(i,j) = (I*K)(i,j) = \sum_{m}\sum_{n}I(m,n)K(i-m,j-n)
\end{equation}

Eine alternative Betrachtungsweise dieser Formel basiert auf dem Hadamard-Produkt. Angenommen, es existieren eine Matrix $I\in\mathbb{R}^{2}$ und eine Matrix $K\in\mathbb{R}^{m\times n}$.

Sei Matrix $H$ definiert als Hadamard-Produkt aus und $K$ und $I'=I_{x, y}$ mit $x\in[i-m,i]$ und $y\in[j-n,j]$.
Dann gilt $S(i,j)=\sum_{h\in H}h$

Diese Formel gilt allerdings nur für Eingabebilder mit nur einem Farbwert pro Pixel, also Graustufenbilder. Für ein Bild mit beliebig vielen Farbwertsdimensionen gilt:

\begin{equation}
S(i,j) = (I*K)(i,j) = \sum_{m}\sum_{n}\sum_{o}I(m,n,o)K(i-m,j-n,o)
\end{equation}

Es ist zu beachten, dass der Farbvektor immer komplett verarbeitet wird, und nicht nur ein Teilausschnitt wie bei Höhe und Breite.

% TODO Hyperparameter

\subsection{Convolutional Layer}
\label{ssec:convlayer}

Klassische Schichten von neuronalen Netzen nutzen Matrixmultiplikationen der kompletten Eingabe mit einer Parametermatrix um ihre Ausgaben zu berechnen, \dh jeder einzelne Ausgabewert entsteht aus einer Berechnung basierend auf jedem einzelnen Eingabewert. Obwohl diese Taktik in vielen Einsatzbereichen gut Funktioniert, stößt sie insbesondere bei der Bilddatenanalyse an ihre Grenzen, da sie nicht gut skaliert. \cite{cs231n} So benötigt eine Convolutional Layer zum Erkennen eines Merkmals nur einen Kernel mit einer Anzahl von meistens unter $\SI{100}{\pixel}$. \cite{deeplearning_16} Ein weiterer Pluspunkt besteht daraus, dass durch Convolutional Layers bestimmte Merkmale von unterschiedlichen Stellen der Eingabe extrahiert werden können, wie später beschrieben.

Eine konvolutionelle Schicht besteht aus einer Menge von Neuron, welche die Eingabe über den eben beschriebenen Konvolutionsoperator $S(i,j)$ auf genau ein Merkmal untersuchen. Daraus folgt, dass die Anzahl der Neuronen in eben dieser Schicht gleich der Größe der entstehenden Merkmalsdimension ist.

Hierbei gilt es zu beachten, dass der Konvolutionsoperator für alle Werte $i$ und $j$ der Eingabedimensionen aufgerufen wird, so dass die Höhe und Breite der Ausgabe identisch zu denen der Eingabe ist.
Des Weiteren ist der Kernel pro Neuron konstant. Dies hat \ua zur Folge, dass das gleiche Merkmal an verschiedenen Stelle in der Eingabe erkennen kann.

Erhält eine konvolutionelle Schicht mit 12 Neuronen also \bspw ein RGB-Eingabebild der Größe $\SI{64}{\pixel}\times\SI{64}{\pixel}$, so erzeugt es mit einem Stride von $1$ als Ausgabe eine dreidimensionale Matrix der Größe $64\times64\times12$, jede der Schichten der dritten Dimensionen deutet auf das Vorhandensein eines speziellen Merkmals an einer gewissen Stelle im Eingabebild hin.

Das Hinzufügen von konvolutionellen Schichten führt allerdings zu mehr Hyperparametern, die optimiert werden können \cite{cs231n}:
\begin{itemize}
	\item Die Dimensionen des Kernels $k_1$ und $k_2$ (obwohl dieser fast immer quadratisch ist, oftmals $3$)
	\item Die Anzahl der Neuronen/der Merkmalsdimensionen
	\item Die Größe der Strides, also wie weit der Kernel in jedem Schritt verschoben wird. Hier wird von einem Wert von $1$ ausgegangen, was die Größe der Ausgabe nicht verändert.
	\item Das Padding, also wie sich die Konvolution an den Rändern der Eingabedaten verhält. Hier wird von Zero-Padding ausgegangen, dies hat zur Folge, dass es die Größe der Ausgabe unverändert lässt.
\end{itemize}

\subsection{Activation Layer}
\label{ssec:activation}

Auf eine konvolutionelle Schicht folgt fast immer eine Aktivierungsschicht. Diese werden dazu genutzt um eine gewisse Nicht-Linearität in den Verlauf des neuronalen Netzes einzuführen. Nicht-Linearität ist in vielen neuronalen Netzen notwendig, da die von ihnen zu lösenden Probleme keine Linearen sind. \cite[Kap.~6]{deeplearning_16}

Selbst wenn nach der letzten Schicht eines mehrschichtigen neuronalen Netzes eine Aktivierungsschicht hinzugefügt werden würde, hätte dies zur Folge, dass die vorherigen Schichten wie eine einzige lineare Schicht agieren, was keine Vorteile gegenüber einer einzigen Schicht hat. Somit sollte jede einzelne Schicht von einer Aktivierungsschicht gefolgt sein, um verschiedenste, komplizierte Zusammenhänge zwischen Ein- und Ausgabedaten besser approximieren zu können.

In \figurename~\ref{fig:activations} sind drei der am meisten Verbreiteten Aktivierungsfunktionen zu sehen.
% TODO Vertikale skalierung egal dank BK??

\begin{figure}[h]
	\captionsetup[subfigure]{labelformat=empty}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\subcaption{ReLU: $f(x)=\mathrm{max}\{0,x\}$}
		\begin{tikzpicture}[scale=0.6]
			\begin{axis}[
				axis lines = middle,
				xlabel = {$x$},
				ylabel = {$f(x)$},
				domain=-5:5,
				]
				\addplot[draw=blue,domain=-5:5,line width=1.5]{max(0,x)};
				\addplot[draw opacity=0,domain=-5:5]{1.2*x};
			\end{axis}
		\end{tikzpicture}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\centering
		\subcaption{Sigmoid: $f(x)=\frac{1}{1+e^{-x}}$}
		\begin{tikzpicture}[scale=0.6]
			\begin{axis}[
				axis lines = middle,
				xlabel = {$x$},
				ylabel = {$f(x)$},
				domain=-5:5,
				]
				\addplot[draw=blue,domain=-5:5,line width=1.5]{1/(1+e^(-x))};
				\addplot[draw opacity=0,domain=-5:5]{0.3*x};
			\end{axis}
		\end{tikzpicture}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\subcaption{tanh: $f(x)=tanh(x)$}
		\begin{tikzpicture}[scale=0.6]
			\begin{axis}[
				axis lines = middle,
				xlabel = {$x$},
				ylabel = {$f(x)$},
				domain=-5:5,
				]
				\addplot[draw=blue,domain=-5:5,line width=1.5]{tanh(x)};
				\addplot[draw opacity=0,domain=-5:5]{0.3*x};
			\end{axis}
		\end{tikzpicture}
	\end{subfigure}
	\caption{Drei Aktivierungsfunktionen}
	\label{fig:activations}
\end{figure}

\subsection{Pooling Layer}
\label{ssec:pooling}

\subsection{Fully Connected Layer}
\label{ssec:fcn}

\subsection{Batch Normalization}
\label{ssec:fcn}

\subsection{Backpropagation}
\label{ssec:backpropagation}