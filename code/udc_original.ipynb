{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JnX34p06RTqm"
   },
   "source": [
    "# Segmentierung der Marsoberfläche mit Hilfe von unüberwachtem tiefem Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zq0mMV3RWGG"
   },
   "source": [
    "## Eingabeparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-iHAO_JQRcBf"
   },
   "outputs": [],
   "source": [
    "input_path = \"images/p03.png\"\n",
    "op_mode = \"tile\" # {batch,tile,single}\n",
    "tile_dims = (640, 625)\n",
    "max_tiles = 5\n",
    "grayscale = True\n",
    "clustering = \"mr\" # {tsugf,slic,lm,s,mr}\n",
    "network = \"mnet\" # {mnet}\n",
    "tensorboard = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl53l5_3RiEj"
   },
   "source": [
    "## Imports und Optionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RjKQm6mNRnbF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import mode\n",
    "from skimage.color import label2rgb\n",
    "from skimage.segmentation import felzenszwalb, slic\n",
    "from sklearn.cluster import KMeans\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "starttime = time()\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter prüfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(input_path) and op_mode != \"batch\":\n",
    "    raise ValueError()\n",
    "if os.path.isfile(input_path) and op_mode not in [\"tile\", \"single\"]:\n",
    "    raise ValueError()\n",
    "if max_tiles is None:\n",
    "    max_tiles = np.inf\n",
    "\n",
    "\n",
    "filelist = []\n",
    "if op_mode == \"single\":\n",
    "    source = cv2.imread(input_path)[:,:,::-1]\n",
    "    images = [source]\n",
    "elif op_mode == \"tile\":\n",
    "    source = cv2.imread(input_path)[:,:,::-1]\n",
    "    images = []\n",
    "    if source.shape[0] % tile_dims[0] != 0 or source.shape[1] % tile_dims[1] != 0:\n",
    "        raise ValueError(\"Image size has to be a divisible by the tile size!\")\n",
    "    for y in range(0, source.shape[0], tile_dims[0]):\n",
    "        for x in range(0, source.shape[1], tile_dims[1]):\n",
    "            images.append(source[y:y + tile_dims[0], x:x + tile_dims[1]])\n",
    "    if max_tiles is not np.inf:\n",
    "        images = images[:max_tiles]\n",
    "elif op_mode == \"batch\":\n",
    "    source = []\n",
    "    filelist = []\n",
    "    ls=os.listdir(input_path)\n",
    "    for file in ls[:]:\n",
    "        try:\n",
    "            source.append(cv2.imread(input_path+\"/\"+file)[:,:,::-1])\n",
    "            filelist.append(file)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UM7bhf5eR3Kw"
   },
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFjrj132R6bU",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if grayscale:\n",
    "    for i in range(len(images)):\n",
    "        images[i] = cv2.cvtColor(images[i], cv2.COLOR_RGB2GRAY)[:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqJMZVVMWZDh"
   },
   "source": [
    "## Clustering Algorithmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tPtJZNAUXsvu"
   },
   "source": [
    "### Gabor Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DKlJzU2xUVYz"
   },
   "outputs": [],
   "source": [
    "def applygaborfilter(A, g, mr=False):\n",
    "  if A.shape[2]==3:\n",
    "      Agray = cv2.cvtColor(A, cv2.COLOR_RGB2GRAY)\n",
    "  elif A.shape[2] == 1:\n",
    "      Agray = A\n",
    "      \n",
    "  numRows = Agray.shape[0]\n",
    "  numCols = Agray.shape[1]\n",
    "  sizeFactors = np.array([4])\n",
    "  #sizeFactors = np.power(2, sizeFactors)\n",
    "  gabormag = np.ndarray((Agray.shape[0], Agray.shape[1], g.shape[2]*len(sizeFactors)))\n",
    "  sigmas = np.ndarray((g.shape[2]+1)*len(sizeFactors))\n",
    "    \n",
    "  #for i in range(g.shape[2]):\n",
    "  #    plt.subplot(15, 15, i+1)\n",
    "  #    plt.imshow(cv2.resize(g[:,:,i], (g[:,:,i].shape[0]*sizeFactors[-1], g[:,:,i].shape[1]*sizeFactors[-1]), interpolation = cv2.INTER_LANCZOS4))\n",
    "  #plt.show()\n",
    "    \n",
    "  for i in range(g.shape[2]):\n",
    "      for s in range(len(sizeFactors)):\n",
    "          gabormag[:, :, i*len(sizeFactors)+s] = cv2.filter2D(Agray, -1, cv2.resize(g[:,:,i], (int(g[:,:,i].shape[0]*sizeFactors[s]), int(g[:,:,i].shape[1]*sizeFactors[s])), interpolation = cv2.INTER_LANCZOS4), borderType=cv2.BORDER_REPLICATE)\n",
    "          sigmas[i*len(sizeFactors)+s] = np.sqrt(2)*g.shape[0]*sizeFactors[s]/49\n",
    "\n",
    "  #for i in range(gabormag.shape[2]):\n",
    "  #    plt.subplot(15, 15, i+1)\n",
    "  #    plt.imshow(gabormag[:, :, i])\n",
    "  #plt.show()      \n",
    "      \n",
    "  for i in range(gabormag.shape[2]):\n",
    "      gabormag[:, :, i] = cv2.GaussianBlur(gabormag[:, :, i], (0, 0), 3*sigmas[i])\n",
    "\n",
    "  X = np.arange(1, numCols + 1)\n",
    "  Y = np.arange(1, numRows + 1)\n",
    "  X, Y = np.meshgrid(X, Y)\n",
    "        \n",
    "  numPoints = numRows * numCols\n",
    "  featureSet = gabormag\n",
    "    \n",
    "  if mr is True:\n",
    "    argsort = np.argsort(featureSet[:,:,:-2].sum(axis=(0,1)))\n",
    "    maxresp = featureSet[:, :, argsort]\n",
    "    maxresp = maxresp[:,:,:(6*len(sizeFactors))]\n",
    "    rotinv = featureSet[:, :, -2:]\n",
    "    featureSet = np.concatenate((maxresp, rotinv), 2)\n",
    "    \n",
    "  if A.shape[2]==3:\n",
    "      featureSet = np.concatenate((featureSet, np.expand_dims(A[:,:,0], axis=2)), 2)\n",
    "      featureSet = np.concatenate((featureSet, np.expand_dims(A[:,:,1], axis=2)), 2)\n",
    "      featureSet = np.concatenate((featureSet, np.expand_dims(A[:,:,2], axis=2)), 2)\n",
    "  elif A.shape[2]==1:\n",
    "      featureSet = np.concatenate((featureSet, np.expand_dims(cv2.GaussianBlur(A[:, :, 0], (0, 0), 15), axis=2)), 2)\n",
    "  \n",
    "  featureSet = np.concatenate((featureSet, np.expand_dims(X, axis=2)), 2)\n",
    "  featureSet = np.concatenate((featureSet, np.expand_dims(Y, axis=2)), 2)\n",
    "\n",
    "  X = featureSet.reshape(numPoints, -1)\n",
    "\n",
    "  #for i in range(X.shape[1]):\n",
    "  #    plt.subplot(15, 15, i+1)\n",
    "  #    plt.imshow(X[:, i].reshape((640,625)))\n",
    "  #plt.show()\n",
    "    \n",
    "  X = X - X.mean(axis=0)\n",
    "  X = X / X.std(axis=0, ddof=1)\n",
    "  X = X[:, ~np.isnan(X).any(axis=0)]\n",
    "  X = X[:, ~np.isinf(X).any(axis=0)]\n",
    "  X = X.reshape(A.shape[0], A.shape[1], -1)\n",
    "\n",
    "  X[:,:,-2:]=  X[:,:,-2:]*0.0\n",
    "  if A.shape[2]==3:\n",
    "    X[:,:,-5:-3]=  X[:,:,-5:-3]*0.75\n",
    "  elif A.shape[2]==1:\n",
    "    X[:,:,-3]=  X[:,:,-3]*0.75\n",
    "    \n",
    "    \n",
    "  X = X.reshape(numPoints, -1)\n",
    "    \n",
    "  L = KMeans(n_clusters=100, n_init=3, max_iter=50, n_jobs=1).fit(X).labels_\n",
    "\n",
    "  return L\n",
    "\n",
    "mat = sio.loadmat(\"./filterbanks/filterbanks.mat\")\n",
    "if clustering == \"tsugf\":\n",
    "  def cluster(image):\n",
    "    return applygaborfilter(image, mat[\"TSUGFfilters\"])\n",
    "if clustering == \"lm\":\n",
    "  def cluster(image):\n",
    "    return applygaborfilter(image, mat[\"LMfilters\"])\n",
    "if clustering == \"s\":\n",
    "  def cluster(image):\n",
    "    return applygaborfilter(image, mat[\"Sfilters\"])\n",
    "if clustering == \"mr\":\n",
    "  def cluster(image):\n",
    "    return applygaborfilter(image, mat[\"RFSfilters\"], mr=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwGZI-D3XykJ"
   },
   "source": [
    "### SLIC Superpixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8x9ibWdIX1hw"
   },
   "outputs": [],
   "source": [
    "if clustering == \"slic\":\n",
    "  def cluster(image):\n",
    "    if images[0].shape[2] == 1:\n",
    "      return slic(image, n_segments=100, compactness=0.1, enforce_connectivity=True)\n",
    "    elif images[0].shape[2] == 3:\n",
    "      return slic(image, n_segments=100, compactness=15, enforce_connectivity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXJ1AA80X48K"
   },
   "outputs": [],
   "source": [
    "if clustering not in [\"tsugf\", \"slic\", \"mr\", \"lm\", \"s\"]:\n",
    "    raise NotImplementedError(\"Clusteringmethode \"+clustering+\" ist nicht implementiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3oAvnXYhCfVn"
   },
   "source": [
    "### Clustermatrix konvertieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVIa84VGCjgl"
   },
   "outputs": [],
   "source": [
    "def gen_cells(tags):\n",
    "  cells = []\n",
    "  for c in np.unique(tags):\n",
    "    cells.append(np.where(c == tags)[0])\n",
    "  return cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6qQztDJLYa4"
   },
   "source": [
    "## Neuronale Netze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnFpHpGSLkGt"
   },
   "source": [
    "### MNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHyBz03LLhRt"
   },
   "source": [
    "#### Architektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3U5mtbIiLnKk"
   },
   "outputs": [],
   "source": [
    "class MNet(nn.Module):\n",
    "  def __init__(self, input_dim, feature_dim):\n",
    "    super().__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.feature_dim = feature_dim\n",
    "    self.fc_dim = feature_dim\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(self.input_dim, self.feature_dim, kernel_size=5, stride=1, padding=2)\n",
    "    self.act1 = nn.ReLU()\n",
    "    self.bn1 = nn.BatchNorm2d(self.feature_dim)\n",
    "    self.pool1 = nn.MaxPool2d(2)\n",
    "    self.conv2a = nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=5, stride=1, padding=2)\n",
    "    self.act2a = nn.ReLU()\n",
    "    self.bn2a = nn.BatchNorm2d(self.feature_dim)\n",
    "    self.pool2a = nn.MaxPool2d(2)\n",
    "    self.conv2b = nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=5, stride=1, padding=2)\n",
    "    self.act2b = nn.ReLU()\n",
    "    self.bn2c = nn.BatchNorm2d(self.feature_dim)\n",
    "    self.conv2c = nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=5, stride=1, padding=2)\n",
    "    self.act2c = nn.ReLU()\n",
    "    self.bn2b = nn.BatchNorm2d(self.feature_dim)\n",
    "    self.conv3 = nn.Conv2d(self.feature_dim, self.fc_dim, kernel_size=1, stride=1, padding=0)\n",
    "    self.bn3 = nn.BatchNorm2d(self.fc_dim)\n",
    "    self.fc1 = nn.Linear(self.fc_dim, self.fc_dim)\n",
    "    self.actf1 = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(self.fc_dim, self.fc_dim)\n",
    "    self.actf2 = nn.ReLU()\n",
    "    # Softmax is already included in the loss function\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.act1(x)\n",
    "    x = self.bn1(x)\n",
    "    #x = self.pool1(x)\n",
    "    x = self.conv2a(x)\n",
    "    x = self.act2a(x)\n",
    "    x = self.bn2a(x)\n",
    "    #x = self.pool2a(x)\n",
    "    x = self.conv2b(x)\n",
    "    x = self.act2b(x)\n",
    "    x = self.bn2b(x)\n",
    "    x = self.conv2c(x)\n",
    "    x = self.act2c(x)\n",
    "    x = self.bn2c(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.bn3(x)\n",
    "    #shape = x.shape\n",
    "    #x = x.view(self.fc_dim, -1)\n",
    "    #x = x.permute(1, 0)\n",
    "    #x = self.fc1(x)\n",
    "    #x = self.actf1(x)\n",
    "    #x = self.fc2(x)\n",
    "    #x = self.actf2(x)\n",
    "    #x = x.permute(1, 0)\n",
    "    #x = x.view(1, self.fc_dim, shape[2], shape[3])\n",
    "    return x[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MQMYE9bPDDL"
   },
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtI-nXWgPGg8"
   },
   "outputs": [],
   "source": [
    "def gen_preview(tags, shape, colors):\n",
    "  return label2rgb(tags.reshape(shape[0], shape[1]), colors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_size(input, network):\n",
    "  model = MNet(input_dim = input.shape[1], feature_dim=100).cuda()\n",
    "  return np.array(model(input).permute(1, 2, 0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuULmVgYMNEc"
   },
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HtDUQmNDMaBM"
   },
   "outputs": [],
   "source": [
    "previews = []\n",
    "labels = []\n",
    "all_tags = []\n",
    "\n",
    "for id in range(min(len(images), max_tiles)):\n",
    "  colors = np.random.randint(255, size=(1000, 3))\n",
    "  model = MNet(input_dim = images[id].shape[2], feature_dim=100).cuda()\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.6)\n",
    "\n",
    "    \n",
    "  if images[id].shape[2] == 3:\n",
    "    preprocess = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "  else:\n",
    "    preprocess = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "    ])\n",
    "  \n",
    "  input = preprocess(images[id].copy()).unsqueeze(0).cuda()\n",
    "\n",
    "  target_shape = get_output_size(input, MNet)\n",
    "  tags= cluster(images[id])\n",
    "\n",
    "    \n",
    "  if target_shape[0] != images[id].shape[0] or target_shape[1] != images[id].shape[1]:\n",
    "    tags = cv2.resize(tags.reshape(images[id].shape[0], images[id].shape[1]), dsize=(target_shape[1], target_shape[0]), interpolation=cv2.INTER_NEAREST).flatten()\n",
    "\n",
    "  cells = gen_cells(tags)\n",
    "    \n",
    "  last_loss = -10\n",
    "  loss_change = []\n",
    "\n",
    "\n",
    "    \n",
    "  if tensorboard:\n",
    "    tb = SummaryWriter(\"{}/{}/{}\".format(\"tensorboard\", starttime, id))\n",
    "    tb.add_image(\"input/\", images[id], 0, dataformats=\"HWC\")\n",
    "    tb.add_image(\"clustered/\", gen_preview(tags, target_shape, colors), 0, dataformats=\"HWC\")\n",
    "\n",
    "    \n",
    "  display(\"+------------+-------------+--------------+------------+-------------------+\")\n",
    "  display(\"|    Tile    |    Epoch    |    Labels    |    Loss    |    Loss Change    |\")\n",
    "  display(\"+------------+-------------+--------------+------------+-------------------+\")\n",
    "\n",
    "  max_epochs = 100\n",
    "  for epoch in range(max_epochs):\n",
    "    raw = model(input)\n",
    "    predicted = raw.permute(1, 2, 0).view(target_shape[0]*target_shape[1], -1)\n",
    "    argmax = torch.argmax(predicted, dim=1).cpu().numpy()\n",
    "    n_labels = len(np.unique(argmax))\n",
    "\n",
    "    target = np.zeros_like(argmax)\n",
    "    for cell in cells:\n",
    "      possible = argmax[cell].flatten()\n",
    "      target[cell] = mode(possible)[0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(predicted, torch.from_numpy(target).cuda().long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    current_loss = loss.item()\n",
    "\n",
    "    display(\"|  {: 4}/{: 4} |  {: 4}/{: 4}  |     {: 4}     |  {: 3.5f}  |     {: 3.7f}    |\".format((id + 1), len(images), epoch + 1, max_epochs, n_labels, current_loss,(current_loss - last_loss) / last_loss))\n",
    "\n",
    "    loss_change.append(np.abs((current_loss - last_loss) / last_loss))\n",
    "\n",
    "    last_loss = current_loss\n",
    "    if tensorboard:\n",
    "      tb.add_scalar(\"loss/\", current_loss, epoch)\n",
    "      tb.add_scalar(\"labels/\", n_labels, epoch)\n",
    "      tb.add_scalar(\"loss_variation/\", loss_change[-1], epoch)\n",
    "      tb.add_image(\"target/\", gen_preview(target, target_shape, colors), epoch, dataformats=\"HWC\")\n",
    "      tb.add_image(\"preview/\", gen_preview(argmax, target_shape, colors), epoch, dataformats=\"HWC\")\n",
    "      tb.flush()\n",
    "\n",
    "    if ((all(c <= -np.inf for c in loss_change[-2:-1]) or loss<=1.7) and epoch >= 5 and n_labels <= 10) or n_labels <= 4:\n",
    "      break\n",
    "    \n",
    "  labels.append(argmax.astype(np.uint8).reshape(target_shape[0], target_shape[1]))\n",
    "  all_tags.append(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sio.savemat(\"data_\"+str(starttime), {\n",
    "    \"filelist\": filelist,\n",
    "    \"mode\": op_mode,\n",
    "    \"clustering\": clustering,\n",
    "    \"images\": images,\n",
    "    \"tags\": all_tags,\n",
    "    \"cells\": cells,\n",
    "    \"labels\": labels,\n",
    "    \"grayscale\": grayscale,\n",
    "    \"source\": source\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "udc.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
